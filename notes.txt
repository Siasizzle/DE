Practical 4
#load csv file
df_iris = spark.read.format("csv").option("multiline", True).option("header",
True).option("inferschema", True).load("/FileStore/tables/Iris-1.csv")
df_iris.show()

df_iris.printSchema()

df_iris.count()

#to print the summary of the columns
df_iris.summary().show()

#select specific columns
df_iris.select("SepalLengthCm","PetalLengthCm").show()

df_iris.createOrReplaceTempView("irisTab")

df_iris1 = spark.sql("select * from irisTab where PetalLengthCm<2.5")
df_iris1.show()

df_iris1.count()

df_iris2 = spark.sql("select * from irisTab where PetalLengthCm between 2.5 and 4.5")
df_iris2.show()

df_iris2.count()

#spark query to see the count of each species
df_iris.groupby("species").count().show()

df_iris1=spark.sql("Select species, count(*) from iristab group by species")
df_iris1.show()

#get the sepal length and sepal width of the species iris-setosa
df_iris2=spark.sql("select SepalLengthCm, SepalWidthCm,species from irisTab where species=\"Iris-setosa\"")
df_iris2.show()

df_iris2 = spark.sql("select species, min(SepalLengthCm) as min_sepal_length from irisTab group by species")
df_iris2.show()

#find out the average petal length of each species in the iris dataset
df_iris.groupby("species").mean("PetalLengthCm").show()

#to run multiple aggregates using spark
spark.sql("select species,min(PetalLengthCm) as min_petal_len,max(PetalLengthCm) as max_petal_len,avg(PetalLengthCm) as avg_petal_len from irisTab group by species").show()

#print the petal length and petal width and species where the averagepetal length is more than 4 
spark.sql("select species,PetalLengthCm,PetalWidthCm from irisTab group by species,PetalLengthCm,PetalWidthCm having mean(PetalLengthCm)>4.0").show()

#to print the data using a spark command where species iris setosa
df_iris.where("species==\"Iris-setosa\"").show()

df_iris2 = df_iris.withColumnRenamed("PetalLengthCm","petal_len")
df_iris2.show()

#conver iris spark dataset to pandas
pd_iris = df_iris.toPandas()
pd_iris.head()

pd_iris.dtypes
Practical 5
rdd_iris1 = sc.textFile("/FileStore/tables/Iris-1.csv",4)
rdd_iris1.collect()

rdd_iris1.getNumPartitions()

rdd_iris1.glom().collect()[0]

len(rdd_iris1.glom().collect()[0])

rdd_iris1.glom().collect()[1]

len(rdd_iris1.glom().collect()[1])

len(rdd_iris1.glom().collect()[2])

len(rdd_iris1.glom().collect()[3])

rdd_iris2 = sc.parallelize(rdd_iris1.collect(),8)
rdd_iris2.getNumPartitions()

for i in rdd_iris1.collect():print(i)

rdd_iris3=rdd_iris2.map(lambda x:x*2)
rdd_iris2.collect()

rdd_iris3.collect()

rdd_iris4 = rdd_iris2.flatMap(lambda x:x.split(","))
rdd_iris4.collect()

rdd_iris4.getNumPartitions()

rdd_iris4.glom().collect()[0]

rdd_iris4.glom().collect()[1]

rdd_iris5 = rdd_iris4.map(lambda x:x*2)
rdd_iris5.collect()

## Filtering
rdd_iris6 = rdd_iris1.flatMap(lambda x:x.split(",")).filter(lambda x:x=="Iris virginica").map(lambda x:(x,1))

rdd_iris6.glom().collect()[1]

rdd_species = rdd_iris1.flatMap(lambda x:x.split(",")).filter(lambda x:(x=="Iris-virginica" or x=="Iris-setosa" or x=="Iris-versicolor"))

rdd_species.collect()

rdd_species1 = rdd_species.groupBy(lambda x:x).mapValues(lambda x:len(x))

rdd_species1.collect()
Practical 6
import numpy as np
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score,classification_report
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split

df_train = pd.read_csv('/home/sumon/Documents/Datasets/titan_train.csv')
df_test = pd.read_csv('/home/sumon/Documents/Datasets/titan_test.csv')

df_train.head()

df_test.head()

df_train.info()

df_test.info()

df_train.isna().sum()

df_test.isna().sum()

def cleaning_data(df):
    m=np.mean(df['Age'])
    df['Age'].fillna(m,inplace=True)
    df['Embarked'].fillna('S',inplace=True)
    df.drop(['Ticket','Name','Cabin','PassengerId'],axis=1,inplace=True)
    m_fare = np.median(df[df['Fare'].isna()==False]['Fare'])
    df['Fare'].fillna(m_fare,inplace=True)
    return df

def encoding_data(df):
    df = pd.get_dummies(data=df,columns=['Sex','Embarked'])
    return df

df_cleaned_tr = cleaning_data(df_train)

df_cleaned_test = cleaning_data(df_test)

df_encode_tr = encoding_data(df_cleaned_tr)
df_encode_test = encoding_data(df_cleaned_test)

logit_p1 = Pipeline([('scaling',StandardScaler()),
                     ('pca',PCA(n_components=3)),
                     ('model_logit',LogisticRegression())])

dtree_p1 = Pipeline([('scaling',StandardScaler()),
                     ('pca',PCA(n_components=3)),
                     ('model_dtree',DecisionTreeClassifier())])

naive_p1 = Pipeline([('scaling',StandardScaler()),
                     ('pca',PCA(n_components=3)),
                     ('model_bayes',GaussianNB())])

x = df_encode_tr.drop('Survived',axis=1)

y = df_encode_tr['Survived']

x_tr,x_test,y_tr,y_test = train_test_split(x,y,test_size=0.2,random_state=100)

my_pipeline = [logit_p1,dtree_p1,naive_p1]

pipeline_dict = {0:'Logistic_Regression',1:'Decision_Tree',2:'Naive_Bayes'}

for i in my_pipeline:
    i.fit(x_tr,y_tr)

for i,model in enumerate(my_pipeline):
    print(f"{pipeline_dict[i]}'s training accuracy is : {model.score(x_tr,y_tr)}")

for i,model in enumerate(my_pipeline):
    print(f"{pipeline_dict[i]}'s testing accuracy is : {model.score(x_test,y_test)}")

Practical 7
from pyspark.ml import Pipeline
from pyspark.ml.feature import StringIndexer,OneHotEncoder,VectorAssembler

df_ipl = spark.read.option("inferSchema",True).option("header",True).csv("/FileStore/tables/ind_ban_comment.csv")
df_ipl.show()

df_ipl.printSchema()

df_ipl_mod = StringIndexer(inputCol="Batsman_Name",outputCol="bt_idx").fit(df_ipl).transform(df_ipl)

df_ipl_mod.select("Batsman_name","bt_idx").show()

df_ipl.columns

df_ipl = df_ipl.drop(*["Batsman","Bowler","Id"])

stage2 = StringIndexer(inputCol='Batsman_Name',outputCol='bm_index')
stage3 = StringIndexer(inputCol='Bowler_Name',outputCol='bowler_index')
stage4 = StringIndexer(inputCols=['bm_index','bowler_index'],outputCols=['bm_ohe','bowler_ohe'])

stage5 = VectorAssembler(inputCols=['Dismissed','Isball','Isboundary','Over','Runs','bm_ohe','bowler_ohe'],outputCol="features")

from pyspark.ml.classification import LogisticRegression
stage6 = LogisticRegression(featuresCol="features",labelCol="Iswicket")

pl = Pipeline(stages=[stage2,stage3,stage4,stage5,stage6])

df_ipl = df_ipl.fillna(0)

logit_model = pl.fit(df_ipl).transform(df_ipl)

logit_model.select("features","Iswicket","prediction").show()

logit_model.columns

pl1 = Pipeline(stages=[stage2,stage3,stage4])

df_mod = pl1.fit(df_ipl).transform(df_ipl)

df_mod.show()

df_vec = VectorAssembler(inputCols=['Dismissed','Isball','Isboundary','Over','Runs','bm_ohe','bowler_ohe'],outputCol="features").transform(df_mod)

df_vector = df_vec.select("features","Iswicket")

splits = df_vector.randomSplit([.7,.3])

df_train = splits[0]
df_test = splits[1]

df_train.show(10)

logit_mod_tr = LogisticRegression(featuresCol="features",labelCol="Iswicket").fit(df_train).transform(df_train)

logit_mod_test = LogisticRegression(featuresCol="features",labelCol="Iswicket").fit(df_train).transform(df_test)

logit_mod_test.select(["features","Iswicket","prediction"]).show()

from pyspark.ml.evaluation import BinaryClassificationEvaluator

eval = BinaryClassificationEvaluator(labelCol="Iswicket",rawPredictionCol="prediction",metricName="areaUnderROC")

eval.evaluate(logit_mod_test)

Practical 8
import pyspark
import pandas as pd
from pyspark import SparkContext,SparkConf
from pyspark.sql import SparkSession

spark_new = SparkSession.builder.master("local[2]").getOrCreate()

data = spark_new.read.option("header",True).option("inferSchema",True).csv("/FileStore/tables/USA_cars_datasets.csv")

data.show()

data.printSchema()

df_panda=data.toPandas()

df_panda.dtypes

df_panda.columns

df_panda.isna().sum()

import seaborn as sns
sns.heatmap(df_panda.corr(), annot=True)

from pyspark.ml.feature import OneHotEncoder
from pyspark.ml.feature import StringIndexer
string_index=StringIndexer(inputCols=['brand','model','color','state'],outputCols=['brand_idx','model_idx','color_idx','state_idx'])

data = string_index.fit(data).transform(data)

data.show()

ohe_mod=OneHotEncoder(inputCols=['brand_idx','model_idx','color_idx','state_idx'],outputCols=['brand_ohe','model_ohe','color_ohe','state_ohe'])
data = ohe_mod.fit(data).transform(data)

from pyspark.ml.feature import VectorAssembler

v1 = VectorAssembler(inputCols=['brand_ohe','color_ohe','state_ohe','year', 'mileage', 'lot'], outputCol="feature_vector")

data_new = v1.transform(data)

data_new.show()

data_new.select("feature_vector").show()

from pyspark.ml.regression import RandomForestRegressor

rf_mod = RandomForestRegressor(featuresCol="feature_vector",labelCol="price")

#cross_validation tuning
from pyspark.ml.tuning import ParamGridBuilder
pm=ParamGridBuilder().addGrid(rf_mod.numTrees,[100,1500]).build()

from pyspark.ml.tuning import CrossValidator
from pyspark.ml.evaluation import RegressionEvaluator
cf = CrossValidator(estimator = rf_mod,
                   estimatorParamMaps = pm,
                   evaluator = RegressionEvaluator(labelCol = "price"),
                   numFolds = 3)

#splitting of the data into train and test set
tr_data,test_data = data_new.randomSplit([.75,.25],seed=100)

data_new.show()

cvmod = cf.fit(tr_data)

bestModel = cvmod.bestModel

bestModel

pred=cvmod.transform(test_data)

pred.show()

pred_mod = pred.select("price","prediction")

pred_mod.show()

pred = pred.withColumnRenamed("price","label")
pred.show()

from pyspark.ml.evaluation import RegressionEvaluator
evaluator = RegressionEvaluator()
eval_score = evaluator.evaluate(pred,{evaluator.metricName:'r2'})

eval_score

Practical 9
import pyspark
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.evaluation import ClusteringEvaluator
import pandas as pd
import seaborn as sns

uberdf_1 = spark.read.option("header",True).option("inferSchema",True).csv("/FileStore/tables/uber_data.csv")
uberdf_1.show()

from pyspark.sql.functions import *
uberdf_1.printSchema()

spark.sql("set spark.sql.legacy.timeParserPolicy=LEGACY")
uberdf_2 = uberdf_1.withColumn("date_new", to_date(col("Date/Time"), "MM/dd/yyyy HH:mm:ss"))
uberdf_2.show()

from pyspark.sql.functions import month,dayofweek,year
uberdf_3 = uberdf_2.withColumn("year", year(col("date_new")))
uberdf_3.show()

uberdf_3.printSchema()

input_features = ["Lat","Lon"]
va = VectorAssembler(inputCols=input_features,outputCol="features")
uberdf_3 = va.transform(uberdf_3)
uberdf_3.show()

df = uberdf_2.toPandas()
df.drop(["Base","Date/Time","date_new"], axis=1, inplace=True)
df

df.head(20)

from sklearn.cluster import KMeans
wcss=[]
for k in range(1,11):
    kmod=KMeans(n_clusters=k, init="random").fit(df)
    wcss.append(kmod.inertia_)

import matplotlib.pyplot as plt
plt.plot(range(1,11), wcss, marker="o", c="blue", markerfacecolor="red")

training, testing = uberdf_3.randomSplit([.75,.25])

training.show()

from pyspark.ml.clustering import KMeans
kmod = KMeans(k=3, initMode="k-means||", featuresCol="features", predictionCol="prediction")
kmod = kmod.fit(training)

p = kmod.transform(testing)
p.show()

pl = p.toPandas()

pl["prediction"].value_counts()

pl.shape

p.createOrReplaceTempView("uber")

df=spark.sql("select prediction, count(*) from uber group by prediction")
df.show()

Practical 10

!pip install keras
!pip install tensorflow
!pip install elephas

from pyspark.ml.feature import StandardScaler, VectorAssembler, StringIndexer, OneHotEncoder
from pyspark.ml import Pipeline
from pyspark.mllib.evaluation import MulticlassMetrics
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
from keras import optimizers, regularizers
from keras.optimizers import Adam
from elephas.ml_model import ElephasEstimator

df = spark.read.option("inferSchema", True).option("header", True).csv("/FileStore/tables/bank_new.csv")
df.show(10)

df.printSchema()

df1 = df.toPandas()

dt = df1.dtypes
numeric = dt[dt=='int32'].index
categorical = dt[dt=='object'].index
categorical

cat_features = dt[dt=="object"].index
cat_features

num_features = dt[dt=="int32"].index
num_features

si = StringIndexer(inputCols = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome', 'deposit'], outputCols = ['job_idx', 'marital_idx', 'education_idx', 'default_idx', 'housing_idx', 'loan_idx', 'contact_idx', 'month_idx', 'poutcome_idx', 'label'])
si

df_si = si.fit(df)
df_si = df_si.transform(df)

df_si.columns

ohe = OneHotEncoder(inputCols = ['job_idx', 'marital_idx', 'default_idx', 'housing_idx', 'loan_idx', 'contact_idx', 'month_idx', 'poutcome_idx'], outputCols = ['job_ohe', 'marital_ohe', 'default_ohe', 'housing_ohe', 'loan_ohe', 'contact_ohe', 'month_ohe', 'poutcome_ohe'])

df_si = si.fit(df).transform(df)

df_si.show()

df_ohe = ohe.fit(df_si).transform(df_si)

df_ohe.select("job_ohe", "marital_ohe").show()

va = VectorAssembler(inputCols = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous', 'job_ohe', 'marital_ohe', 'default_ohe', 'housing_ohe', 'loan_ohe', 'contact_ohe', 'month_ohe', 'poutcome_ohe'], outputCol="features")

df_new = va.transform(df_ohe)

d1 = df_new.select("features").toPandas()
d1.head()

from pyspark.sql.functions import rand

df_new = df_new.select("features", "label")
df_new = df_new.orderBy(rand())
df_new.show()

df2 = df_new.toPandas()

df2.head()

df_new.select("features").show(5)

df_tr, df_test = df_new.randomSplit([.8, .2])
df_tr.show()

n_classes = df_tr.select("label").distinct().count()
n_classes

input_dim = len(df_tr.select("features").first()[0])
input_dim

model = Sequential()
model.add(Dense(64, input_shape=(input_dim,), activity_regularizer = regularizers.l2(0.01)))
model.add(Activation('relu'))
model.add(Dropout(rate=0.3))
model.add(Dense(64, activity_regularizer=regularizers.l2(0.01)))
model.add(Activation('relu'))
model.add(Dropout(rate=0.3))
model.add(Dense(n_classes))
model.add(Activation('sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam')
optimizer_conf = optimizers.Adam(learning_rate=0.01)
opt_conf = optimizers.serialize(optimizer_conf)

#Initialize SparkML Estimator and get Settings
estimator = ElephasEstimator()
estimator.setFeaturesCol("features")
estimator.setLabelCol("label")
estimator.set_keras_model_config(model.to_json())
estimator.set_categorical_labels(True)
estimator.set_nb_classes(n_classes)
estimator.set_num_workers(1)
estimator.set_epochs(25)
estimator.set_batch_size(64)
estimator.set_verbosity(1)
estimator.set_validation_split(0.10)
estimator.set_optimizer_config(opt_conf)
estimator.set_mode("synchronous")
estimator.set_loss("binary_crossentropy")
estimator.set_metrics(['acc'])

dl_pipeline = Pipeline(stages=[estimator])
fit_pipeline = dl_pipeline.fit(df_new)

df_new.printSchema()

pred_train = fit_pipeline.transform(df_new)
pred_train.show()

pnl_train = pred_train.select("label", "prediction")
pnl_train.printSchema()

pnl_train.show()

import numpy as np
df = pnl_train.toPandas()
li_pred = []
for i in df["prediction"]:
    li_pred.append(np.argmax(i))
df["pred_new"] = li_pred
df.head()

from sklearn.metrics import confusion_matrix, accuracy_score
print(confusion_matrix(df["label"], df["pred_new"]))

accuracy_score(df["label"], df["pred_new"])
